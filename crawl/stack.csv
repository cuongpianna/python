title,question
How to filter Pandas rows based on last/next row?,"
I have two data sets from different pulse oximeters, and plot them with pyplot as displayed below. As you may see, the green data sheet has alot of outliers(vertical drops). In my work I've defined these outlayers as non-valid in for my statistical analysis, they are must certainly not measurements. Therefore I argue that I can simply remove them. 
The characteristics of these rogue values is that they're single(or top two) value outliers(see df below). The ""real"" sample values are either the same as the previous value, or +-1. In e.g. java(pseudo code) I would do something like:
for(i; i <df.length; i++)
  if (df[i+1|-1].spo2 - df[i].spo2 > 1|-1)
    df[i].drop

What would be the pandas(numpy?) equivalent of what I'm trying to do, remove values that is more/less than 1 compared to the last/next value?

df:
    time, spo2
1900-01-01 18:18:41.194  98.0
1900-01-01 18:18:41.376  98.0
1900-01-01 18:18:41.559  78.0
1900-01-01 18:18:41.741  98.0
1900-01-01 18:18:41.923  98.0
1900-01-01 18:18:42.105  90.0
1900-01-01 18:18:42.288  97.0
1900-01-01 18:18:42.470  97.0
1900-01-01 18:18:42.652  98.0

 
"
Fast way to place list elements on 2D array,"
I have two lists, both lists of lists. One contains values and one that contains the corresponding positions in a 2D array where those values lie. For example
values    = [[5,1,1,8,10],[3,1,7,9,4]
positions = [[(0,0),(1,2),(1,4),(3,3),(4,4)],[(0,1),(1,3),(2,4),(3,4),(4,0)]]

I want to put these values in their corresponding positions on a 2D array of zeros. So I made the following function
import itertools

def list_to_array(n_1, n_2, positions, values):

    new_array_list = []

    for k in range(len(positions)):

        A = np.zeros((n_1,n_2))

        for i, j in itertools.product(range(n_1), range(n_2)):

            if (i,j) in positions:

                X = positions.index((i,j))
                A[i,j] = values[X]

        new_array_list += [A]

    return new_array_list

For a 256x256 array, a single iteration takes roughly one second and I need to perform the iteration for 1600 lists, which is long but still reasonable. However, I was also hoping to do this for larger arrays (up to 8x the length in each direction), so it is obvious the above code would just take too long. I was wondering if anyone knows of a faster way to do this?
"
conda commands cannot work correctly,"
I met some problems after installing Anaconda3.
 
My operation system is Win 10;
Anaconda version is Anaconda3-5.2.0-Windows-x86_64;
Installation location is : C:\Anaconda
 
When I type “conda”, “python”, ”pip” or “--version”, they work correctly.
But if I type “conda list” or “conda update conda ”, it has some errors as followed:
 
>>>>>>>>>>>>>>>>>>>>>>ERROR REPORT<<<<<<<<<<<<<<<<<<<<<<
Traceback (most recent call last):
      File ""C:\Anaconda\lib\site-packages\conda\cli\main.py"", line 97, in main
        from ..activate import main as activator_main
      File ""C:\Anaconda\lib\site-packages\conda\activate.py"", line 12, in 
        context.init()  # oOn import, context does not include SEARCH_PATH. This line fixes that.
.
.
.
 File ""C:\Anaconda\lib\site-packages\ruamel_yaml\reader.py"", line 241, in update
        self.check_printable(data)
      File ""C:\Anaconda\lib\site-packages\ruamel_yaml\reader.py"", line 208, in check_printable
        'unicode', ""special characters are not allowed"")
    ruamel_yaml.reader.ReaderError: unacceptable character #x0000: special characters are not allowed
      in ""C:\Users\martin.condarc"", position 0
 
I think the reason might be something about unicode, I tried to find the answer on google and stack overflow, unfortunately, I still cannot solve it.
"
how to delete user input command after reply,"
Python Bot not deleting user input command after sending reply. i added await bot.delete_message(message) in last line still its not deleting after replying.
example: after bot reply's Pong. it should delete ?ping
@bot.command(pass_context=True)
async def ping(ctx):
    msg = ""Pong. {0.author.mention}"".format(ctx.message)
    await bot.say(msg)
    await bot.delete_message(message)

"
"python selenium firefox, special character @ for authentication popup","
I'm trying to log in with python Selenium to a webpage that wants authentication credentials.
Unlike the example here
my username is an email address, so it contains the '@' character (and I'm using Python).
from selenium import webdriver
driver = webdriver.Firefox()
webpage = 'http://somewhere.com/cgi-bin/dirwrap.cgi?template=template&path=news'
username = 'myname@mymailbox.com'
password = 'mypassword'
url = username + ':' + password + '@' + webpage
print(url)
driver.get(url)

output is
myname@mymailbox.com:mypassword@http://somewhere.com/cgi-bin/dirwrap.cgi?template=template&path=news
Traceback (most recent call last):
File ""question.py"", line 12, in <module>
  driver.get(url)
File ""/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 324, in get
  self.execute(Command.GET, {'url': url})
File ""/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 312, in execute
  self.error_handler.check_response(response)
File ""/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
  raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidArgumentException: Message: Malformed URL: myname@mymailbox.com:mypassword@http://somewhere.com/cgi-bin/dirwrap.cgi?template=template&path=news is not a valid URL.

I believe the problem is from the extra '@' character.  How do I put it in without creating a malformed URL?
"
Checking out and getting file details using pysvn,"
I'm trying to check out few folders from SVN and also their latest revisions with their SVN path to use further down my program. I'm able to check out all the folders using the following code:
client = pysvn.Client()
client.checkout(source,dest)

And using client.info2() I am able to get the revisions from the checked out copy on my system. Is there any way that the details of the file can directly be exported from SVN?
Thanks
"
Why “os.path.join” and “os.path.dirname(__file__)” return different separator?,"
I'm a beginner.
My system is win10 Pro,and I use python3.X.
I use this code to test function ""os.path.join()"" and ""os.path.dirname()"".
import os
print(os.path.join(os.path.dirname(__file__), ""dateConfig.ini""))

The output is:

E:/test_opencv\dateConfig.ini

I found os.path.join() use ""/"",but os.path.dirname() use ""\"",why?
If I want to use the same separator,all is '/' or '\',what should I do?
"
Pandas drop before first valid index and after last valid index for each column of a dataframe,"
I have a dataframe like this:
df = pd.DataFrame({'timestamp':pd.date_range('2018-01-01', '2018-01-02', freq='2h', closed='right'),'col1':[np.nan, np.nan, np.nan, 1,2,3,4,5,6,7,8,np.nan], 'col2':[np.nan, np.nan, 0, 1,2,3,4,5,np.nan,np.nan,np.nan,np.nan], 'col3':[np.nan, -1, 0, 1,2,3,4,5,6,7,8,9], 'col4':[-2, -1, 0, 1,2,3,4,np.nan,np.nan,np.nan,np.nan,np.nan]
              })[['timestamp', 'col1', 'col2', 'col3', 'col4']]

which looks like this:
             timestamp  col1  col2  col3  col4
0  2018-01-01 02:00:00   NaN   NaN   NaN  -2.0
1  2018-01-01 04:00:00   NaN   NaN  -1.0  -1.0
2  2018-01-01 06:00:00   NaN   0.0   NaN   0.0
3  2018-01-01 08:00:00   1.0   1.0   1.0   1.0
4  2018-01-01 10:00:00   2.0   NaN   2.0   2.0
5  2018-01-01 12:00:00   3.0   3.0   NaN   3.0
6  2018-01-01 14:00:00   NaN   4.0   4.0   4.0
7  2018-01-01 16:00:00   5.0   NaN   5.0   NaN
8  2018-01-01 18:00:00   6.0   NaN   6.0   NaN
9  2018-01-01 20:00:00   7.0   NaN   7.0   NaN
10 2018-01-01 22:00:00   8.0   NaN   8.0   NaN
11 2018-01-02 00:00:00   NaN   NaN   9.0   NaN

Now, I want to find an efficient and pythonic way of chopping off (for each column! Not counting timestamp) before the first valid index and after the last valid index. In this example I have 4 columns, but in reality I have a lot more, 600 or so. I am looking for a way of chop of all the NaN values before the first valid index and all the NaN values after the last valid index.
One way would be to loop through I guess.. But is there a better way? This way has to be efficient. I tried to ""unpivot"" the dataframe using melt, but then this didn't help. 
An obvious point is that each column would have a different number of rows after the chopping. So I would like the result to be a list of data frames (one for each column) having timestamp and the column in question. For instance:
             timestamp  col1   
3  2018-01-01 08:00:00   1.0  
4  2018-01-01 10:00:00   2.0   
5  2018-01-01 12:00:00   3.0   
6  2018-01-01 14:00:00   NaN   
7  2018-01-01 16:00:00   5.0   
8  2018-01-01 18:00:00   6.0   
9  2018-01-01 20:00:00   7.0   
10 2018-01-01 22:00:00   8.0    

My try
I tried like this:
final = []
columns = [c for c in df if c !='timestamp']
for col in columns:
    first = df.loc[:, col].first_valid_index()
    last = df.loc[:, col].last_valid_index()
    final.append(df.loc[:, ['timestamp', col]].iloc[first:last+1, :])

"
Why does url disappear from python scrapy?,"
Why does url disappear from python scrapy?
disappear_url = 'https://disqus.com/embed/comments/?base=default&f=zdnet-1&t_i=5d1b4eb7-9581-4eaa-acb9-5fbc6c15bdb7&t_u=https%3A%2F%2Fwww.zdnet.com%2Fproduct%2Fsamsung-chg9-series-c49hg90dmn-qled-monitor-curved-49%2F&t_e=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&t_d=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&t_t=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&s_o=default#version=48537a333e429dcb726ce9cdcc57a44f'

I tried this.
scrapy shell disppear_url

response.url
https://disqus.com/embed/comments/?base=default&f=zdnet-1&t_i=5d1b4eb7-9581-4eaa-acb9-5fbc6c15bdb7&t_u=https%3A%2F%2Fwww.zdnet.com%2Fproduct%2Fsamsung-chg9-series-c49hg90dmn-qled-monitor-curved-49%2F&t_e=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&t_d=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&t_t=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&s_o=default

response.request.url
https://disqus.com/embed/comments/?base=default&f=zdnet-1&t_i=5d1b4eb7-9581-4eaa-acb9-5fbc6c15bdb7&t_u=https%3A%2F%2Fwww.zdnet.com%2Fproduct%2Fsamsung-chg9-series-c49hg90dmn-qled-monitor-curved-49%2F&t_e=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&t_d=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&t_t=Samsung%2049%20inch%20QLED%20business%20monitor%20review%3A%20Multi-tasking%20single%20monitor%20setup&s_o=default#version=48537a333e429dcb726ce9cdcc57a44f

help me.
"
opening another file from python,"
newquiz = input (""would you like to start a different quiz?"")
if newquiz == (""yes"") or start == (""yes"").upper() or start == (""Yes""):
    exec (""N:\A Level work\computing coursework\quiz2.py"")

so basically, in this code, if the user enters yes, then the code will open up my second python quiz which is in the same folder. However, when run, the code returns a syntax error: invalid syntax(,line 1). Help would be greatly appreciated.
"
An error occurred while calling o38.load,"
I'm running a pyspark script which uses a third party package.But it raises an error saying:

py4j.protocol.Py4JJavaError: An error occurred while calling o38.load
  java.lang.NoClassDefFoundError: com/mongodb/ConnectionString

But there is no further description. So I'm finding it hard to figure out what this error is all about. I have also searched google but couldn't find anything. Anyone knows what this error is?
"
Import pcap on raspberry,"
I'm trying to load the module pcap of the python library  pypcap 1.2.2, but when i try to import it, i got the following error: 
import pcap
Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""build/bdist.linux-armv6l/egg/pcap.py"", line 7, in <module>
   File ""build/bdist.linux-armv6l/egg/pcap.py"", line 6, in __bootstrap__
ImportError: /root/.cache/Python-Eggs/pypcap-1.2.2-py2.7-linux-armv6l.egg-tmp/pcap.so: undefined symbol: pcap_set_immediate_mode

my system info are:
Linux raspberrypi 3.19.3+ #35 PREEMP armv6l GNU/Linux 

Any idea?
"
Pandas: VLOOKUP partial string match,"
I am trying to perform an action in Python which is very similar to VLOOKUP in Excel. There have been many questions related to this on StackOverflow but they are all slightly different from this use case. Hopefully anyone can guide me in the right direction. I have the following two pandas dataframes:
df1 = pd.DataFrame({'Invoice': ['20561', '20562', '20563', '20564'],
                    'Currency': ['EUR', 'EUR', 'EUR', 'USD']})
df2 = pd.DataFrame({'Ref': ['20561', 'INV20562', 'INV20563BG', '20564'],
                    'Type': ['01', '03', '04', '02'],
                    'Amount': ['150', '175', '160', '180'],
                    'Comment': ['bla', 'bla', 'bla', 'bla']})

print(df1)
    Invoice Currency
0   20561   EUR
1   20562   EUR
2   20563   EUR
3   20564   USD

print(df2)
    Ref         Type    Amount  Comment
0   20561       01      150     bla
1   INV20562    03      175     bla
2   INV20563BG  04      160     bla
3   20564       02      180     bla

Now I would like to create a new dataframe (df3) where I combine the two based on the invoice numbers. The problem is that the invoice numbers are not always a ""full match"", but sometimes a ""partial match"" in df2['Ref']. So the joining on 'Ref' does not give the desired output because it doesn't copy the data for invoices 20562 & 20563, see below:
df3 = df1.join(df2.set_index('Ref'), on='Invoice')

print(df3)
    Invoice Currency    Type    Amount  Comment
0   20561   EUR         01       150    bla
1   20562   EUR         NaN      NaN    NaN
2   20563   EUR         NaN      NaN    NaN
3   20564   USD         02       180    bla

Is there a way to join on a partial match? I know how to ""clean"" df2['Ref'] with regex, but that is not the solution I am after. With a for loop, I get a long way but this isn't very Pythonic.
df4 = df1.copy()
for i, row in df1.iterrows():
    tmp = df2[df2['Ref'].str.contains(row['Invoice'])]
    df4.loc[i, 'Amount'] = tmp['Amount'].values[0]

print(df4)
Invoice     Currency    Amount
0   20561   EUR         150
1   20562   EUR         175
2   20563   EUR         160
3   20564   USD         180

Can str.contains() somehow be used in a more elegant way? Thank you so much in advance for your help!
"
How do I sum time series data by day in Python? resample.sum() has no effect,"
I am new to Python. How do I sum data based on date and plot the result?
I have a Series object with data like:
2017-11-03 07:30:00      NaN
2017-11-03 09:18:00      NaN
2017-11-03 10:00:00      NaN
2017-11-03 11:08:00      NaN
2017-11-03 14:39:00      NaN
2017-11-03 14:53:00      NaN
2017-11-03 15:00:00      NaN
2017-11-03 16:00:00      NaN
2017-11-03 17:03:00      NaN
2017-11-03 17:42:00    800.0
2017-11-04 07:27:00    600.0
2017-11-04 10:10:00      NaN
2017-11-04 11:48:00      NaN
2017-11-04 12:58:00    500.0
2017-11-04 13:40:00      NaN
2017-11-04 15:15:00      NaN
2017-11-04 16:21:00      NaN
2017-11-04 17:37:00    500.0
2017-11-04 21:37:00      NaN
2017-11-05 03:00:00      NaN
2017-11-05 06:30:00      NaN
2017-11-05 07:19:00      NaN
2017-11-05 08:31:00    200.0
2017-11-05 09:31:00    500.0
2017-11-05 12:03:00      NaN
2017-11-05 12:25:00    200.0
2017-11-05 13:11:00    500.0
2017-11-05 16:31:00      NaN
2017-11-05 19:00:00    500.0
2017-11-06 08:08:00      NaN

I have the following code:
# load packages
import pandas as pd
import matplotlib.pyplot as plt

# import painkiller data
df = pd.read_csv('/Users/user/Documents/health/PainOverTime.csv',delimiter=',')

# plot bar graph of date and painkiller amount
times = pd.to_datetime(df.loc[:,'Time'])

ts = pd.Series(df.loc[:,'acetaminophen'].values, index = times,
               name = 'Painkiller over Time')
ts.plot()

This gives me the following line(?) graph:

It's a start; now I want to sum the doses by date. However, this code fails to effect any change: The resulting plot is the same. What is wrong?
ts.resample('D',closed='left', label='right').sum()
ts.plot()

I have also tried ts.resample('D').sum(), ts.resample('1d').sum(), ts.resample('1D').sum(), but there is no change in the plot. 
Is .resample even the correct function? I understand resampling to be sampling from the data, e.g. randomly taking one point per day, whereas I want to sum each day's values.
Namely, I'm hoping for some result (based on the above data) like:
2017-11-03 800
2017-11-04 1600
2017-11-05 1900
2017-11-06 NaN

"
extracting second table from HTML using Selenium na python,"
I am using Selenium and python to extract table from HTML but I am able to extract only first table. I want to extract second table also I am new t selenium.
Html code
<div class=""fondo-top"">
        <h3>Flood Forecasted Site</h3>
        <div class=""editorwys"">



        <h4>Site Name : Araria</h4> 
                <table style=""width: 100%"">
                    <tbody><tr>
                        <th style=""width: 25%""><strong>District Name:</strong></th>
                        <td style=""width: 25%""><strong>Araria</strong></td>

                            <th style=""width: 25%""><strong>Warning Level (WL):</strong></th>
                            <td style=""width: 25%""><strong> Meters (m)</strong></td>

                    </tr>
                    <tr>
                        <th><strong>River Name:</strong></th>
                        <td><strong>Mahananda</strong></td>

                            <th><strong>Danger Level (DL):</strong></th>
                            <td><strong> Meters (m)</strong></td>

                    </tr>
                    <tr>
                        <th><strong>Basin Name:</strong></th>
                        <td><strong>GANGA<strong></strong></strong></td>

                        <th><strong>Highest Flood Level (HFL):</strong></th>
                        <td><strong>49.4 Meters (m)<strong></strong></strong></td>

                    </tr>
                    <tr>
                        <th><strong>Division Name:</strong></th>

                        <td><a target=""_blank"" href=""/opencms/opencms/ffs/Detail?code=029-mgd4ptn "" onclick=""window.open(this.href, this.target, 'width=1000, height=600, toolbar=no, resizable=no'); return false;"">Lower Ganga Division-I(LGD-I), Patna</a></td>

                        <th><strong>HFL Attained date:</strong></th>
                        <td>14-08-2017</td>

                    </tr>
                </tbody></table>
                <p>&nbsp;</p>
                <table>
                    <tbody><tr>
                        <th colspan=""3"" style=""text-align: center;""><strong>PRESENT WATER LEVEL</strong></th>
                    </tr>

                    <tr>                            

                        <td class="""" style=""width:33%; height:18px;"">Date: 22-06-2018 08:00</td>
                        <td class="""" style=""width:33%;"">Value: 45.34 Meters (m)</td>
                        <td class="""" style=""width:33%;"">Trend: Steady</td>
                    </tr>
                    <tr>
                        <th colspan=""3"" style=""text-align: center;""><strong>CUMULATIVE DAILY RAINFALL</strong></th>
                    </tr>
                    <tr>

                            <td style=""width:33%; height:18px;"">Date: 22-06-2018 08:30</td>
                            <td style=""width:33%;"">Value: 5.0 Milimiters (mm)</td>
                            <td style=""width:33%;""></td>

                    </tr>
                </tbody></table>                            
                    <p>&nbsp;</p>                       



                                <table style=""width: 100%"">
                                    <tbody><tr>
                                        <th colspan=""4"" style=""text-align: center;""><strong>NO FLOOD FORECAST</strong></th>
                                    </tr>
                                </tbody></table>



            <div class=""botonera"">

                <a href=""/ffs/data-flow-list-based/"" class=""mas-info""> Go Back</a>

            </div>
    </div>
</div>

I am using driver.find_element_by_css_selector('table > tbody').text to extract the first table. Now how to extract the second and third table table??? I will realy appriciate any help in this regard
"
How to get a value from a cell of a dataframe?,"
I have constructed a condition that extract exactly one row from my data frame:
d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]

Now I would like to take a value from a particular column:
val = d2['col_name']

But as a result I get a data frame that contains one row and one column (i.e. one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?
"
Discretization size and Recurrence period of numpy uniform,"
My queries are regarding the generation of the uniform random number generator using numpy.random.uniform on [0,1).

Does this implementation involve a uniform step-size, i.e. are the universe of possibilities {0,a,2a,...,Na} where (N+1)a = 1 and a is constant?
If the above is true, then what's the value of this step-size? I noticed that the value of numpy.nextafter(x,y) keeps on changing depending upon x. Hence my question regarding whether a uniform step-size was used to implement numpy.random.uniform.
If the step-size is not uniform, then what would be the best way to figure out the number of unique values that numpy.random.uniform(low=0, high=1) can take?
What's the recurrence period of numpy.random.uniform, i.e. after how many samples will I see my original number again? For maximum efficiency, this should be equal to the number of unique values.

I tried looking up the source code at Github but didn't find anything directly interpretable there.
"
Python-create table from string,"

Is it possible to create this table in Python, i have bash code for this but it does't work in python
""description"":""Patch the following servers for any\\u00a0Windows Updates and Application Updates.\\r\\n\\r\\nSQL Updates or rollups will not be done. All the servers to be put in maintenance mode on Zabbix before commencing the maintenance on all the servers.\\u00a0Once the update or maintenance is completed, the table below will be completed to show the status of the updates and added to JIRA comments.\\r\\n\\r\\nPlease review infrastructure for new servers and add to the list.\\r\\n\\r\\n\\u00a0\\r\\n|Server Name|OS Updates Completed|Application Updates Completed (list)|Remarks|\\r\\n|PEL-ACD-01A|\\u00a0Yes/No|\\u00a0Java, Chrome etc|\\u00a0|\\r\\n|PEL-ACD-01B|\\u00a0|\\u00a0|\\u00a0|\\r\\n|PEL-JMP-01A|\\u00a0|\\u00a0|\\u00a0|\\r\\n|PEL-SQL-01A|\\u00a0|\\u00a0|\\u00a0|\\r\\n|PEL-TLS-01A|\\u00a0|\\u00a0|\\u00a0|""

This is what i get using python

"
How to convert video dataset to TFRecords?,"
I have a big video training dataset to process and I want to convert it to a TFRecords file. How to I convert my variable write into a TFRecords file? This variable is a list of sequences where for each sequence I have a given number of frames which is the path for each one.
I found some example on how to do it with images but I could not find anything that used sequences of images. My goal is to generate a batch of sequences.
"
passing an array with index of discrete feature to mutual_info_classif in python,"
I am using MI from sklearn.feature_selection.mutual_info_classif to calculate MI between 4 continuous variables(X matrix) and y(target class)
X:
prop_tenure prop_12m    prop_6m prop_3m
    0.04        0.04        0.06    0.08
    0           0           0       0
    0           0           0       0
    0.06        0.06        0.1     0
    0.38        0.38        0.25    0
    0.61        0.61        0.66    0.61
    0.01        0.01        0.02    0.02
    0.1         0.1         0.12    0.16
    0.04        0.04        0.04    0.09
    0.22        0.22        0.22    0.22
    0.72        0.72        0.73    0.72
    0.39        0.39        0.45    0.64

**y**

status
0
0
1
1
0
0
0
1
0
0
0
1

So my X is all continuous and y is discrete.
There is a parameter in the function to which I can pass the index of discrete features:
sklearn.feature_selection.mutual_info_classif(X, y, discrete_features=’auto’, n_neighbors=3, copy=True, random_state=None)

and I am doing as below:
print(mutual_info_classif(X,y,discrete_features = [3],n_neighbors = 20))
[0.12178862 0.12968448 0.15483147 0.14721018]

Though this is not giving error, I am not sure if I am passing the right index for identifying the y variable as discrete and others as continuous.
Can someone please clarify if I am wrong?
"
How can I use scikit-learn in R?,"
I am doing a clustering task using meanshift in R.
The shape of the data is 2D with amount of over 5000 sampling.
I have tried R package MeanShift and MeanShiftR; However, both of them are slow and took me more than 5 hours. I tried it on python using sklearn, and it finished in seconds. 
I want to write as few python code as possible(so the code still can be maintainable by R people) to run sklearn in R.
"
Creating/Getting/Extracting multiple data frames from python dictionary of dataframes,"
I have a python dictionary with keys as dataset names and values as the entire data frames themselves, see the dictionary dict below 
[Dictionary of Dataframes ]
One way id to write all the codes manually like below:
csv = dict['csv.pkl']
csv_emp = dict['csv_emp.pkl']    
csv_emp_yr= dict['csv_emp_yr.pkl']    
emp_wf=dict['emp_wf.pkl']
emp_yr_wf=dict['emp_yr_wf.pkl']

But this will get very inefficient with more number of datasets.
Any help on how to get this done over a loop? 
"
query based on test cases of competitive proframming questions [on hold],"
I have been practising programming but want to excel more for that i need some help related to test case in competitive programming questions which are being asked in 'hacker earth' and 'code chef' , etc..
I want to know how to pass all the test cases in nut shell how to solve them  
"
Dynamic class variable for Marshmallow Schema?,"
I'm making several Schemas using Marshmallow. Some of them would be UserSchema, PatientSchema and AppointmentSchema. I'm also making a ""overall"" FeedbackSchema, looking like this:
class FeedbackSchema(Schema):
    success = fields.Bool()
    message = fields.String()
    error = fields.String()
    data = ?

I imagine that when a end user asks for the route /users/all it will result in a FeedbackSchema where the data field is filled with a list of all users. That is, fields.List(fields.Nested(UserSchema)). However if they go to /patient/1 the data field is filled with a single patient, that is fields.Nested(PatientSchema).
What I want is to not have to create several different FeedbackSchemas being virtually identical, except in their definition of the type of the data class variable.
How can I achieve this dynamically? Some sort of class factory I'm not aware of?
"
Django access model data in flatpages,"
Is it possible to access data from the DB and use it on a flatpage? I want to access the values stored in my table and display it on the flatpage. I've already created templates/flatpages/default.html under my app's directory.
"
How to append column name to the left of Dataframe,"
Good evening guys,
I am working with pandas to learn Data Science. I work with the data on Kaggle Stack Overflow 2018 Developer Survey. I seen an code which display the amount of NaN data from Lathwal. 
The Dataframe has 3 Columns but 2 are assigned a column name and I want to add at the beginning a Column name. I was browsing the pandas docs but so far I seen just merging 2 Dataframes or more. I think there is a easier way to accomplish this Task.
total = stack_data.isnull().sum().sort_values(ascending = False)
percent = (stack_data.isnull().sum()/stack_data.isnull().count()*100).sort_values(ascending = False)
missing_stack_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_stack_data

I get:
                             Absolute Frequency  Relative Frequency
TimeAfterBootcamp                         92203           93.270952
MilitaryUS                                83074           84.036215
HackathonReasons                          73164           74.011431
ErgonomicDevices                          64797           65.547519

I would like to have:
Specific columnename                Absolute Frequency  Relative Frequency
TimeAfterBootcamp                         92203           93.270952
MilitaryUS                                83074           84.036215
HackathonReasons                          73164           74.011431
ErgonomicDevices                          64797           65.547519

Thank you in Advance
Theeninfam
"
How to read messages after the last consumed message in case of consumer shutdown in Kafka?,"
I have a way of performing this task in shell:
How to make kafka consumer to read from last consumed offset but not from beginning
However, I am willing to do this in Python, using kafka-python
I could not find any api for this case.
http://kafka-python.readthedocs.io/en/latest/apidoc/KafkaConsumer.html
"
chromedriver not found error on heroku app,"
I want to use selenium on my heroku app. So I added following buildpacks. 

Following is the python code which uses the selenium, chrome and chromedriver
from selenium import webdriver

# from selenium.webdriver.chrome.options import Options

import os

import pickle

from selenium.webdriver.support.ui import WebDriverWait

from selenium.webdriver.chrome.options import Options as ChromeOptions

chrome_bin = os.environ.get('GOOGLE_CHROME_SHIM', None)
opts = ChromeOptions()
opts.binary_location = chrome_bin
driver = webdriver.Chrome(executable_path=""chromedriver"", 
chrome_options=opts)

I am getting following error
FileNotFoundError: [Errno 2] No such file or directory: 'chromedriver': 'chromedriver'
Please help me.
"
How to save best weights of the encoder part only during auto-encoder training??,"
I am using keras with tensor flow to implement a deep auto-encoder with CNN:
So basically the model would be similar to: 
    input_data = Input(shape=(40,500,1))

    #encoder  
    x= Conv2D(32,kernel_size=(3,3), padding=""same"",activation='linear')(input_data)       
    encoded= Conv2D(15,kernel_size=(1,2), strides=(1,2), padding=""same"",activation='linear')(x)  


    #decoder             
    x= Conv2DTranspose(15,kernel_size=(1,2), padding=""same"",activation='linear')(encoded)        
    x= Conv2DTranspose(32,kernel_size=(3,3), padding=""same"",activation='linear')(x)
    decoded = Conv2DTranspose(1, (3, 3), activation=activationfuntion, padding=""same"")(x)

    autoencoder = Model(inputs=input_data,outputs=decoded)
    encoder = Model(inputs=input_data,outputs=encoded)  

In order to save the best model weights during training, I am using ModelCheckpoint:
        autoencoder.compile(loss='mean_squared_error', optimizer='rmsprop');

        checkpoint=ModelCheckpoint('bestweight.best.hdf5',monitor='val_loss',verbose=1,save_best_only=True,mode='min');   
        callbacks_list=[checkpoint]

        history_info =autoencoder.fit(x_train, x_train,
                        batch_size=batch_size,
                        epochs=50,
                        validation_data=(x_validation,x_validation),
                        callbacks=callbacks_list,
                        shuffle=True)

and then later to test on the testdataset:
 autoencoder.load_weights('bestweight.best.hdf5');
 autoencoder.predict(test_data);

My question is:
I know how to save the best weights of the whole auto-encoder, but is there a way to just save the best training weights of the encoder part so I can use it later for testing.
so I can use it in this way:
 encoder.load_weights('encoderbestweight.best.hdf5');
 encoder.predict(test_data);

"
Django- Change Username field to BigAutoField?,"
I'm designing an Application where username will be an IntegerField(optional can be a typecasted to string) and unique.
Here's my model.
class ModelA(models.Model):
    username = models.BigAutoField(primary_key=True, db_index=False)
    user_id = models.UUIDField(default=uuid.uuid4, unique=True,
                               editable=False)

Initially, I wanted user_id to be a primary_key, but I can't create an AutoField which is not primary_key. As a result, I'd to let go off user_id as primary_key and assigned username as the primary key.
Now, when I run the migrations, it throws an error saying,
django.db.utils.ProgrammingError: operator class ""varchar_pattern_ops"" does not accept data type bigint

"
Dataframe is working but not able to view as table in Jupyter,"
I am working on a NLP project where we analyse a lot of text data. I am facing a weird problem where my dataframe is not loading when I want to see in Jupyter notebook. It is giving formatting error. 
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
D:\Program\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

D:\Program\Anaconda3\lib\site-packages\IPython\lib\pretty.py in pretty(self, obj)
    393                             if callable(meth):
    394                                 return meth(obj, self, cycle)
--> 395             return _default_pprint(obj, self, cycle)
    396         finally:
    397             self.end_group()

D:\Program\Anaconda3\lib\site-packages\IPython\lib\pretty.py in _default_pprint(obj, p, cycle)
    508     if _safe_getattr(klass, '__repr__', None) is not object.__repr__:
    509         # A user-provided repr. Find newlines and replace them with p.break_()
--> 510         _repr_pprint(obj, p, cycle)
    511         return
    512     p.begin_group(1, '<')

D:\Program\Anaconda3\lib\site-packages\IPython\lib\pretty.py in _repr_pprint(obj, p, cycle)
    699     """"""A pprint that just redirects to the normal repr function.""""""
    700     # Find newlines and replace them with p.break_()
--> 701     output = repr(obj)
    702     for idx,output_line in enumerate(output.splitlines()):
    703         if idx:

D:\Program\Anaconda3\lib\site-packages\pandas\core\base.py in __repr__(self)
     78         Yields Bytestring in Py2, Unicode String in py3.
     79         """"""
---> 80         return str(self)
     81 
     82 

D:\Program\Anaconda3\lib\site-packages\pandas\core\base.py in __str__(self)
     57 
     58         if compat.PY3:
---> 59             return self.__unicode__()
     60         return self.__bytes__()
     61 

D:\Program\Anaconda3\lib\site-packages\pandas\core\frame.py in __unicode__(self)
    634             width = None
    635         self.to_string(buf=buf, max_rows=max_rows, max_cols=max_cols,
--> 636                        line_width=width, show_dimensions=show_dimensions)
    637 
    638         return buf.getvalue()

D:\Program\Anaconda3\lib\site-packages\pandas\core\frame.py in to_string(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, line_width, max_rows, max_cols, show_dimensions)
   1673                                            max_cols=max_cols,
   1674                                            show_dimensions=show_dimensions)
-> 1675         formatter.to_string()
   1676 
   1677         if buf is None:

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in to_string(self)
    595         else:
    596 
--> 597             strcols = self._to_str_columns()
    598             if self.line_width is None:  # no need to wrap around just print
    599                 # the whole frame

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _to_str_columns(self)
    523                 str_columns = [[label] for label in self.header]
    524             else:
--> 525                 str_columns = self._get_formatted_column_labels(frame)
    526 
    527             stringified = []

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _get_formatted_column_labels(self, frame)
    772                             need_leadsp[x] else x]
    773                            for i, (col, x) in enumerate(zip(columns,
--> 774                                                             fmt_columns))]
    775 
    776         if self.show_index_names and self.has_index_names:

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in <listcomp>(.0)
    771             str_columns = [[' ' + x if not self._get_formatter(i) and
    772                             need_leadsp[x] else x]
--> 773                            for i, (col, x) in enumerate(zip(columns,
    774                                                             fmt_columns))]
    775 

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _get_formatter(self, i)
    355         else:
    356             if is_integer(i) and i not in self.columns:
--> 357                 i = self.columns[i]
    358             return self.formatters.get(i, None)
    359 

D:\Program\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in __getitem__(self, key)
   1741 
   1742         if is_scalar(key):
-> 1743             return getitem(key)
   1744 
   1745         if isinstance(key, slice):

IndexError: index 4 is out of bounds for axis 0 with size 4

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
D:\Program\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    343             method = get_real_method(obj, self.print_method)
    344             if method is not None:
--> 345                 return method()
    346             return None
    347         else:

D:\Program\Anaconda3\lib\site-packages\pandas\core\frame.py in _repr_html_(self)
    667 
    668             return self.to_html(max_rows=max_rows, max_cols=max_cols,
--> 669                                 show_dimensions=show_dimensions, notebook=True)
    670         else:
    671             return None

D:\Program\Anaconda3\lib\site-packages\pandas\core\frame.py in to_html(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, bold_rows, classes, escape, max_rows, max_cols, show_dimensions, notebook, decimal, border)
   1732                                            decimal=decimal)
   1733         # TODO: a generic formatter wld b in DataFrameFormatter
-> 1734         formatter.to_html(classes=classes, notebook=notebook, border=border)
   1735 
   1736         if buf is None:

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in to_html(self, classes, notebook, border)
    731                                       border=border)
    732         if hasattr(self.buf, 'write'):
--> 733             html_renderer.write_result(self.buf)
    734         elif isinstance(self.buf, compat.string_types):
    735             with open(self.buf, 'w') as f:

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in write_result(self, buf)
   1214         indent += self.indent_delta
   1215         indent = self._write_header(indent)
-> 1216         indent = self._write_body(indent)
   1217 
   1218         self.write('</table>', indent)

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _write_body(self, indent)
   1377                 self._write_hierarchical_rows(fmt_values, indent)
   1378             else:
-> 1379                 self._write_regular_rows(fmt_values, indent)
   1380         else:
   1381             for i in range(min(len(self.frame), self.max_rows)):

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in _write_regular_rows(self, fmt_values, indent)
   1411             row = []
   1412             row.append(index_values[i])
-> 1413             row.extend(fmt_values[j][i] for j in range(ncols))
   1414 
   1415             if truncate_h:

D:\Program\Anaconda3\lib\site-packages\pandas\io\formats\format.py in <genexpr>(.0)
   1411             row = []
   1412             row.append(index_values[i])
-> 1413             row.extend(fmt_values[j][i] for j in range(ncols))
   1414 
   1415             if truncate_h:

KeyError: 0

I am able to perform all the operation on data and it is loading as individual column. Once I face such condition then a small sample dataframe is also giving same error. Seems, its something related Jupyter format rendering.
When I restart and run the Kernal then same data is loading fine but this is not practical solution as I can't restart everytime, my text data processing takes a lot of time. I don't know why I face this problem all of sudden and the same code is running when I restart the Kernal. Any help will be really appreciated.
"
how to send password while executing python script on a remote machine,"
I have a python script which I want to execute on a remote machine.
I am doing it using below command
import os

cmd=""cat myonboarding.py | ssh cloud-user@10.80.99.45 python""

os.system(cmd)

How ever while executing the script, password prompt pops up.
How to send password here?
Regards,
Sridevi
"
Error in building opencv.js in windows,"
I did change in statements as provided in OSError: [WinError 193] %1 is not a valid Win32 application opencv.js, but its giving a different error now. It shows
PS C:\Users\Chintu\Documents\OpenCv\opencv_back> python ./platforms/js/build_js.py build_js
Traceback (most recent call last):
File ""C:\Users\Chintu\Documents\OpenCv\emsdk\emscripten\1.38.6\tools\cache.py"", line 137, in 
from . import shared
ImportError: cannot import name 'shared'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
from . import jsrun, cache, tempfiles, colored_logger

File ""C:\Users\Chintu\Documents\OpenCv\emsdk\emscripten\1.38.6\tools\cache.py"", line 140, in 
   import shared
ImportError: No module named 'shared'
Traceback (most recent call last):
File ""./platforms/js/build_js.py"", line 215, in <module>
builder = Builder(args)
File ""./platforms/js/build_js.py"", line 84, in __init__
self.emcc_version = determine_emcc_version(self.emscripten_dir)
File ""./platforms/js/build_js.py"", line 62, in determine_emcc_version
ret = subprocess.check_output([os.path.join(emscripten_dir, ""emcc.bat""), ""--version""])
File ""C:\Python34\lib\subprocess.py"", line 620, in check_output
raise CalledProcessError(retcode, process.args, output=output)
subprocess.CalledProcessError: Command '['C:\\Users\\Chintu\\Documents\\OpenCv\\emsdk\\emscripten\\1.38.6\\emcc.bat', '--version']' returned non-zero exit status 1
Can you help me out!!
"
Two part form on single page in django,"
How would I go about making a two part form on a single page? As in the form would have different tabs to switch between different views of the the form all while on the same page? could this be done with AJAX?
I'm using Django and python 3.4 currently.
Example
"
Django how to show available item only in CBV List view?,"
I have two models car and booking. A car is ForeignKey field in booking model if car is booked then in car list its still showing. I have is_available_car model field in car model default=True. If car is booked then it will not show till it's unbooked. i am having booking form and if car is booked user cannot book it and it doesnt shown in CArlistview  . Help me please. Thank you.
Models
 class Booking(models.Model):

 booking_name = models.CharField(max_length=240, null=False)
customer_name = models.ForeignKey(Customer, on_delete=models.CASCADE, related_name='book_customers' )
book_car = models.ForeignKey(Car, on_delete=models.CASCADE, related_name='book_car')
booking_start_date = models.DateTimeField(auto_now_add=True, blank=False)
booking_end_date = models.DateTimeField(blank=True, null=True)
rental_price = models.IntegerField(blank=False, null=False)
times_pick = models.TimeField(blank=True)
is_approved = models.BooleanField(default=False)
def __str__(self):
    return self.booking_name

def get_absolute_url(self):
    return reverse(""buggy_app:detail"",kwargs={'pk':self.pk})

Views.py
    class BookingView(FormView):
        template_name = 'buggy_app/booking.html'
        form_class = BookingForm
        models = Booking
        def form_valid(self, form):
        form.save()
        return super(BookingView, self).form_valid(form)

        success_url = reverse_lazy('index')

        def get_context_data(self, **kwargs):
        # kwargs['car'] is the car booking now!
        try:
            kwargs['car'] = Car.objects.get(id=self.request.GET.get('car', 
             ''))
       except (Car.DoesNotExist, ValueError):
             kwargs['car'] = None
             return super(BookingView, self).get_context_data(**kwargs)

        def get_initial(self):
            initial = super(BookingView, self).get_initial()
            if 'car' in self.request.GET:
            try:
                initial['book_car'] = 
                Car.objects.get(id=self.request.GET['car'])
           except (Car.DoesNotExist, ValueError):
                pass
                return initial

            class CarListView(ListView):
                 context_object_name = 'cars'
                 model = models.Car

                 def get_queryset(self):
                 qs = super(CarListView, self).get_queryset()
                 qs = qs.filter(is_available_car=True)
                 return qs

"
"Python, pip: avoid gcc during installation","
Our local python package server contains these files:
subprocess32-3.2.7-cp27-cp27mu-linux_x86_64.whl
subprocess32-3.5.0-cp27-none-linux_x86_64.whl
subprocess32-3.5.0rc1-cp27-none-linux_x86_64.whl
subprocess32-3.5.0.tar.gz
subprocess32-3.5.2.tar.gz

The file subprocess32-3.5.2.tar.gz is new.
Before installing of subprocess32 was successful, since this new version exists, it fails. It fails because there is no gcc on the machine which tries to install subprocess32.
What can I do? I think there are these solutions.

remove subprocess32-3.5.2.tar.gz
make subprocess32-3.5.2 available as wheel 
make gcc available on the machine
fix the dependency to subprocess32-3.5.0

But all of them don't really make me happy, since I only solve my current problem. Some weeks later, the same thing can happen again.
Is there a way to tell pip to use a wheel even if this means to take an older version?
Background: there is no explicit dependency on the new version. Pip tries to take the latest version.
"
How to randomly duplicate some documents while importing the data set?,"
I have imported a data set (20 newsgroup) in my model, but at the same time I also want to create randomly duplicates of some of the documents while importing.
from sklearn.datasets import fetch_20newsgroups

categories = [
    'alt.atheism',
    'talk.religion.misc',
    'comp.graphics',
    'sci.space',
]
print(""Loading 20 newsgroups dataset for categories:"")
data_train = fetch_20newsgroups(subset='train', categories=categories,
                            shuffle=True, random_state=42)

data_test = fetch_20newsgroups(subset='test', categories=categories,
                           shuffle=True, random_state=42)
print('data loaded')

Does anyone knows what changes to be made to get that?
"
Count Partial match in each DataFrame Column in Pandas,"
I have Data frame in which I want to count occurrence of some word per each column.
Per one coumn I can do:
df['Col1'].str.contains('test').value_couns()

or
df[df['Col1'].str.contains('test')]['Col1'].count()

and i get count for particular column.
How can I get it for all columns?
I would like to avoid do it manually per each column since there are quite a few of them.

Expected output

"
Response.error - 310 UNKNOWN STATUS CODE,"
I'm getting response.error specifically for 1 user of my site.
The status code is 310 UNKNOWN STATUS CODE
Any resolution for this?
"
Extract numbers by position in Pandas?,"
I have a df:
                  col1
0       01139290201001
1       01139290101001
2       01139290201002
3       01139290101002
4       01139290201003
5       01139290101003
6       01139290201004
7       01139310101001
8       01139290201005
9       01139290301001
            ...      
5908      01139ÅÊ21020
5909      01139ÅÊ21013
5910      01139ÅÊ11008
5911      01139ÅÊ21011
5912      01139ÅÊ03003

and I need to extract to a new column the first 7 numbers in the int only cases and the first 5 and 8,9 numbers in the cases where characters are included.
I tried this code to a made up dataframe to try out ways to solve it and it worked but when I tried it on the actual dataset it didn't work as expected with the main reason being that my actual df has integers and it did calculations on them.
df['col2']=df[col1][0:5]+df['col1'][8]


0       0113929020100101139290201005
1       0113929010100101139290201005
2       0113929020100201139290201005
3       0113929010100201139290201005
4       0113929020100301139290201005
5                                NaN
6                                NaN
7                                NaN
8                                NaN
9                                NaN

also why it causes NaN values?
i want it to look like this:
 01139290201001 to 0113929 for integer only rows and like this for the others
 01139ÅÊ03003 to 0113903

"
Can you calculate a current value from a cumulative metric?,"
I've written a small python script that polls the metrics from EMQTT and feeds them into graphite.
However all the metrics from EMQTT are cumulative ""since the cluster started"" values. I'm not sure what value that has to whom, but what would be useful is current values for the metrics.
I'm not all that interested in how many messages have ever been received, or how many connections have ever been established - I want to know how many messages there are right now, and how many connections are currently active.
Is there any way to calculate this from the cumulative values ? I thought about doing it by merely, on each poll, subtracting the current value from the last one I read - but this has a problem. Lets say I get 5 connections on this poll, and remember it.
Value: 5
Now two people disconnect, and 6 more connect.
Next value: 11 (5 + 6)
If I subtract 5 from 11 it says there are now 6 connections. But that's not true, three connections persisted from the previous poll - so the actual value should be 9 now.
Honestly I'm not even sure if calculating this is mathematically possible. But if it is, I would love to be able to extract that value if it somehow is. At least with this approach errors aren't cumulative - but they could be significant nonetheless. All it really lets me track is the change-in-value over time. Which may STILL be more useful on a graph than the cumulative value.
So first prize is if one of the built-in functions in grafana can do this calculation - if so, please tell me it's name, if not a proper current value -the change-in-value-between-points would still be a more useful graph than the cumulative total. 
Second prize, but I'll take it if I can get it, is an algorithm to calculate the proper value myself- the code is in python but I'm happy to write it myself from pseudo code if somebody can just tell me what the calculation should actually look like.
"
Pytorch broadcasting product of two tensors,"
I want to multiply two tensors, here is what I have got:
A tensor of shape (20, 96, 110)
B tensor of shape (20, 16, 110)
The first index is for batch size.
What I want to do is essentially take each tensor from B - (20, 1, 110), for example, and with that, I want to multiply each A tensor (20, n, 110).
So the product will be at the end: tesnor AB which shape is (20, 96 * 16, 110).
So I want to multiply each tensor from A by broadcasting with B.
Is there a method in pytorch that does it?
"
How to randomize quick sort in python?,"
I am implementing randomized quick sort. For now I have created a function ChoosePivot(A,N). This returns a random pivot and its location in input array. Then I switch that random pivot with first element in array so that pivot in Partition (A,l,r) is always first element. For Now ChoosePivot(A,N) is also returning first element of the array but I plan to modify it later. 
Following is my code:
def QuickSort(A,N):
    if (N == 1):
        return
    pivot, pivot_pos = ChoosePivot(A,N)
    l = 0
    r = len(A)
    # Preprocessing, swapping pivot position with first element so that first element remains pivot always
    temp = A[0]
    A[0] = A[pivot_pos]
    A[pivot_pos] = temp

    A, i= Partition(A,l,r)
    print A,i
    # If i-1 == 0 this means that there is no left subarray
    if (i-1 != 0):
        print ""Unsorted array""
        print A[0:i-1]
        QuickSort(A[0:i-1],i-1)
        print ""Left call""
        print A

    if (N-i !=0): 
        print ""Unsorted array""
        print A[i:N]
        QuickSort (A[i:N], N-i)
        print ""Right call""
        print A

Following is my Partition(A,l,r)
def Partition(A,l,r):
    # Now first element is the pivot
    i= l + 1
    pivot = A[l]
    for j in range(l+1, r):
        if (A[j] < pivot):
            #swap (A[j], A[i])
            temp_1 = A[i]
            A[i] = A[j]
            A[j] = temp_1
            i = i+1
    #swap (A[i-1], A[l])
    temp_2 = A[i-1]
    A[i-1] = A[l]
    A[l] = temp_2
    return A, i 

ChoosePivot(A,N) Simply returns the first element in array for now
def ChoosePivot(A, N):
    #print A
    return A[0], 0

Input array I used is as follows:
Test_in = [3,8,2,5,1,4,7,6]
print Test_in

QuickSort(Test_in, len(Test_in))
print Test_in 

Please see that I can see code working at lower end of recursion. I did dry run code with pen and paper and I can see with print statements that it does sort the sub arrays but when array is returned finally, it has not changed. I thought its something related to value vs  reference calling but I found that python calls by reference. So there should not be any problem there. 
Posting output as well and pointing out exactly where it went wrong.

"
unable to log into amazon prime video to scrape movie titles using requests,"
I'm trying to log into amazon prime video using python requests. I need help with the parameters required to log in and are cookies and session needs to be managed? please help.
code : 
import requests
from bs4 import BeautifulSoup

loginUrl =""https://www.amazon.com/ap/signin?accountStatusPolicy=P1&clientContext=261-2859886-1913604&language=en_US&openid.assoc_handle=amzn_prime_video_desktop_us&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&openid.ns.pape=http%3A%2F%2Fspecs.openid.net%2Fextensions%2Fpape%2F1.0&openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.primevideo.com%2Fauth%2Freturn%2Fref%3Dav_auth_ap%3F_encoding%3DUTF8%26location%3D%252Fref%253Ddv_auth_ret""

homeUrl = ""https://www.primevideo.com/storefront/home/ref=topnav_storetab_atv""

payload = {'email':'something@gmail.com',
       'password': 'something',}

header={'User-Agent' : 'Mozilla/5.0'}

with requests.Session() as s:
    r = s.get(loginUrl)


    p = s.post(loginUrl, data=payload, headers = header)

    print( p.text)

    r = s.get(homeUrl)

    print (r.text)

output :
'please enable cookies in browser' html page. basically i want to log into the login page and then ""get"" home page html.
"
pipe.recv throws EOFError,"
I'm using sphinx for python documentation.
The parallel build fails with an EOFError (Serial build works fine.). I can't add a reproducer here but it all boils down to below execution of pipe.recv() in in _join_one of sphinx code:
(As long as this line shows the right thing - https://github.com/sphinx-doc/sphinx/blob/master/sphinx/util/parallel.py#L119)
def _join_one(self):
    # type: () -> None
    for tid, pipe in iteritems(self._precvs):
        if pipe.poll():
            exc, logs, result = pipe.recv() # Throws EOFError in some of the threads

The recv fails consistently.
Any idea as to what might be causing the recv to fail with EOFError?
I guess the pipe is closed when pipe.recv is executed but that would throw an [IOError: [Errno 9] Bad file descriptor], but doesn't seem to be the case here.
"
How to fetch only the url links from the string in python? [duplicate],"

This question already has an answer here:


Find string between two substrings

                    19 answers
                


Match text between two strings with regular expression

                    2 answers
                



How do I fetch only the links from my file shown below:
Jun 15 16:26:21 dnsmasq[1979]: query[A] fd-geoycpi-uno.gycpi.b.yahoodns.net from 192.168.1.33 

Jun 15 16:26:30 dnsmasq[1979]: query[A] armdl.adobe.com from 192.168.1.24

Jun 15 16:26:32 dnsmasq[1979]: query[A] updates.installshield.com from 192.168.1.118

Note: the links may or may not start with ""www."" or end with "".com"" (example: armdl.adobe.com, fd-geoycpi-uno.gycpi.b.yahoodns.net) but the ""query[A]"" before the link and ""from"" after the link remains same for every string. Thank you.
"
pyqt5 QTranslator() load not work after compile via pyinstaller but in pycharm run it's work,"
I make i18n style practice on a little test project (use pyqt5), IDE is pycharm.
When I try pycharm run application then check the i18n switch language, it's work. But when I use pyinstaller compiler to a single or folder style 
the application does not work.
I guess maybe cause not load the qm file. The qm file content is about i18n language word string (on the sample project I use two language English and Chinese).
Even If I try to place the qm file on the same path with application, it still not load the qm file.
My project (include ui file and ts + qm file).
I also make a video to record my operate : my video demo
Win64 + win32 + mac64, all got the same problem.
"
Make flat list from a list of strings each evaluable as a list,"
For example, how would I merge:
res_str = ['[1,2,3]','[4,5,6]','[7,8,9]','[10,11,12]']

into:
[1,2,3,4,5,6,7,8,9,10,11,12]

I used the following code, but is is not fast enough:
[x for j in res_str for x in eval(j)]

Is there a better way to write this?
apart from a generator
(x for j in res_str for x in eval(j))

"
How can I add a field's length for the sort in Model?,"
How can I add a field's length for the sort in Model?
I have an IPv4Manage model:
class IPv4Manage(models.Model):
    """"""
    ipv4
    """"""
    ip = models.GenericIPAddressField(help_text=""ip"")
    ...
    class Meta:
        ordering = ['ip']

In the Meta we can set the ordering criteria with the fields.
But I have a question, can I set the ip's length for order in the Model?
I know if in the APIView I can use Prefetch or extra for the length of the ip and order it.
...extra(select={'length':'Length(ip)'}).order_by('length', 'ip')

But can we set the length of field param of order in the Model?
"
Django- url not displaying page despite changing,"
This is the index.html
{% extends 'base.html' %}
{% block content %}
<h1>list of  {{title}}</h1>
{% if questions %}
    <ul>
        {% for question in questions %}
        <li>
           <a href=""/polls/{{question.id}}/details/""> {{question.title}}</a>

        </li>
        {% endfor %}
    </ul>
        {% else %}
        <p>there is a no question available</p>
        {% endif %}


{% endblock %}

details.html
{% extends 'base.html' %}
{% block content %}
<H2>Details Page</H2>
<h3>{{question.title}}</h3>
{% for choice in question.choices %}
<p>{{choice.text}}({{choice.votes}})</p>
{% empty %}
<p>There is no choice available for this Question</p>
{% endfor %}


<p>Poll is created by {{question.created_by.first_name}}</p>

{% endblock %}

This is base.html
{%load static%}
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <title>WebPage</title>
    <link rel=""stylesheet"" href=""(%static 'css/custom.css'%)"">
</head>
<body>
<h1>Welcome to My Page</h1>
{%  block content %}
{% endblock %}
</body>
</html>

And poll.urls
from django.conf.urls import url
from poll.views import *
urlpatterns = [
    url('', index, name='polls_list'),
    url('<int:id>/details/', details, name='polls_details')
]

So this is what happens I have 3 questions that display on the index.html, on clicking any of them it changes to the proper url but does not open the requested page. How do I solve this. Do keep in mind that i just started django two days ago so pardon my naivety.Thanks
"
unable to log into amazon prime video to scrape movie titles using requests,"
I'm trying to log into amazon prime video using python requests. I need help with the parameters required to log in and are cookies and session needs to be managed? please help.
code : 
import requests
from bs4 import BeautifulSoup

loginUrl =""https://www.amazon.com/ap/signin?accountStatusPolicy=P1&clientContext=261-2859886-1913604&language=en_US&openid.assoc_handle=amzn_prime_video_desktop_us&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&openid.ns.pape=http%3A%2F%2Fspecs.openid.net%2Fextensions%2Fpape%2F1.0&openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.primevideo.com%2Fauth%2Freturn%2Fref%3Dav_auth_ap%3F_encoding%3DUTF8%26location%3D%252Fref%253Ddv_auth_ret""

homeUrl = ""https://www.primevideo.com/storefront/home/ref=topnav_storetab_atv""

payload = {'email':'something@gmail.com',
       'password': 'something',}

header={'User-Agent' : 'Mozilla/5.0'}

with requests.Session() as s:
    r = s.get(loginUrl)


    p = s.post(loginUrl, data=payload, headers = header)

    print( p.text)

    r = s.get(homeUrl)

    print (r.text)

output :
'please enable cookies in browser' html page. basically i want to log into the login page and then ""get"" home page html.
"
pipe.recv throws EOFError,"
I'm using sphinx for python documentation.
The parallel build fails with an EOFError (Serial build works fine.). I can't add a reproducer here but it all boils down to below execution of pipe.recv() in in _join_one of sphinx code:
(As long as this line shows the right thing - https://github.com/sphinx-doc/sphinx/blob/master/sphinx/util/parallel.py#L119)
def _join_one(self):
    # type: () -> None
    for tid, pipe in iteritems(self._precvs):
        if pipe.poll():
            exc, logs, result = pipe.recv() # Throws EOFError in some of the threads

The recv fails consistently.
Any idea as to what might be causing the recv to fail with EOFError?
I guess the pipe is closed when pipe.recv is executed but that would throw an [IOError: [Errno 9] Bad file descriptor], but doesn't seem to be the case here.
"
How to fetch only the url links from the string in python? [duplicate],"

This question already has an answer here:


Find string between two substrings

                    19 answers
                


Match text between two strings with regular expression

                    2 answers
                



How do I fetch only the links from my file shown below:
Jun 15 16:26:21 dnsmasq[1979]: query[A] fd-geoycpi-uno.gycpi.b.yahoodns.net from 192.168.1.33 

Jun 15 16:26:30 dnsmasq[1979]: query[A] armdl.adobe.com from 192.168.1.24

Jun 15 16:26:32 dnsmasq[1979]: query[A] updates.installshield.com from 192.168.1.118

Note: the links may or may not start with ""www."" or end with "".com"" (example: armdl.adobe.com, fd-geoycpi-uno.gycpi.b.yahoodns.net) but the ""query[A]"" before the link and ""from"" after the link remains same for every string. Thank you.
"
pyqt5 QTranslator() load not work after compile via pyinstaller but in pycharm run it's work,"
I make i18n style practice on a little test project (use pyqt5), IDE is pycharm.
When I try pycharm run application then check the i18n switch language, it's work. But when I use pyinstaller compiler to a single or folder style 
the application does not work.
I guess maybe cause not load the qm file. The qm file content is about i18n language word string (on the sample project I use two language English and Chinese).
Even If I try to place the qm file on the same path with application, it still not load the qm file.
My project (include ui file and ts + qm file).
I also make a video to record my operate : my video demo
Win64 + win32 + mac64, all got the same problem.
"
Make flat list from a list of strings each evaluable as a list,"
For example, how would I merge:
res_str = ['[1,2,3]','[4,5,6]','[7,8,9]','[10,11,12]']

into:
[1,2,3,4,5,6,7,8,9,10,11,12]

I used the following code, but is is not fast enough:
[x for j in res_str for x in eval(j)]

Is there a better way to write this?
apart from a generator
(x for j in res_str for x in eval(j))

"
How can I add a field's length for the sort in Model?,"
How can I add a field's length for the sort in Model?
I have an IPv4Manage model:
class IPv4Manage(models.Model):
    """"""
    ipv4
    """"""
    ip = models.GenericIPAddressField(help_text=""ip"")
    ...
    class Meta:
        ordering = ['ip']

In the Meta we can set the ordering criteria with the fields.
But I have a question, can I set the ip's length for order in the Model?
I know if in the APIView I can use Prefetch or extra for the length of the ip and order it.
...extra(select={'length':'Length(ip)'}).order_by('length', 'ip')

But can we set the length of field param of order in the Model?
"
Django- url not displaying page despite changing,"
This is the index.html
{% extends 'base.html' %}
{% block content %}
<h1>list of  {{title}}</h1>
{% if questions %}
    <ul>
        {% for question in questions %}
        <li>
           <a href=""/polls/{{question.id}}/details/""> {{question.title}}</a>

        </li>
        {% endfor %}
    </ul>
        {% else %}
        <p>there is a no question available</p>
        {% endif %}


{% endblock %}

details.html
{% extends 'base.html' %}
{% block content %}
<H2>Details Page</H2>
<h3>{{question.title}}</h3>
{% for choice in question.choices %}
<p>{{choice.text}}({{choice.votes}})</p>
{% empty %}
<p>There is no choice available for this Question</p>
{% endfor %}


<p>Poll is created by {{question.created_by.first_name}}</p>

{% endblock %}

This is base.html
{%load static%}
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <title>WebPage</title>
    <link rel=""stylesheet"" href=""(%static 'css/custom.css'%)"">
</head>
<body>
<h1>Welcome to My Page</h1>
{%  block content %}
{% endblock %}
</body>
</html>

And poll.urls
from django.conf.urls import url
from poll.views import *
urlpatterns = [
    url('', index, name='polls_list'),
    url('<int:id>/details/', details, name='polls_details')
]

So this is what happens I have 3 questions that display on the index.html, on clicking any of them it changes to the proper url but does not open the requested page. How do I solve this. Do keep in mind that i just started django two days ago so pardon my naivety.Thanks
"
How to merge dictionaries in for loop - Python,"
I am trying to merge dictionaries returning from for loop. I have to then take maximum total from the dictionary. Kindly help me, I am new to python. 
enter image description here
"
get word type using wordnet in python,"
How can I find the type of words using nltk Python module and WordNet?
For example I have words like ""Sushi/Apple"" and the type of these words is ""Food"". How can I get this using nltk and WordNet in Python? 
"
How get value from aggregations in Elasticsearch dsl,"
I began to understand with Elasticsearch dsl and faced with problem. I can not get value from aggregations. I is use the metric method, but is get a error, when I want to get the value from 'new_distance'. What doing wrong?
    res = MyIndex.search()\
      .filter('geo_distance', distance='1km', location={""lat"":""32.895"", ""lon"":""21.115""})\
      .aggs.metric('new_distance', 'max', field='point')
    result = res.to_queryset()
    print(result[0].new_distance)

error
'Comments' object has no attribute 'new_distance'

"
Sending matrices to a Tensorflow-Serving module,"
I am trying to send 3 dense matrices to a tensorflow-serving module. This module should take in 3 dense matrices and return a list of floats. I'm doing this in Python.
I am having serious difficulty figuring out how to do this. I'm not sure how to send more than one piece of data at a time.
I looked at this link and it looked like it would be helpful?
https://www.tensorflow.org/serving/signature_defs
But I'm not sure.
I was previously looking at this website as a resource: https://towardsdatascience.com/deploy-tensorflow-models-9813b5a705d5
However, 

req_data = [{'in_tensor_name': 'inputs', 'in_tensor_dtype': 'DT_FLOAT', 'data': np.random.rand(1,2)}]
prediction = client.predict(req_data, request_timeout=10)

does not seem analogous to what I'm trying to do.
Sorry if this question is pretty vague, but I have really no idea where to start. I have these 3 matrices, and apparently the model I'm sending these matrices to receives 3 matrices and outputs one list of numbers. How do I send and receive the data from this model?
Thanks
"
Unable to connect to mysql with pymysql,"
I'm trying to connect to mysql from pymysql with the following example script shared on their repo, but it keeps failing with the below message.
  File ""/Users/svenkatesh/Library/Python/2.7/lib/python/site-packages/pymysql/connections.py"", line 818, in _connect
    2003, ""Can't connect to MySQL server on %r (%s)"" % (self.host, e))
pymysql.err.OperationalError: (2003, ""Can't connect to MySQL server on 'localhost' (255)"")

Here's mysql user configuration, I tried both for root and rv,
mysql> select user, host, plugin from user;
+------------------+-----------+-----------------------+
| user             | host      | plugin                |
+------------------+-----------+-----------------------+
| mysql.infoschema | localhost | mysql_native_password |
| mysql.session    | localhost | mysql_native_password |
| mysql.sys        | localhost | mysql_native_password |
| root             | localhost | caching_sha2_password |
| rv               | localhost | mysql_native_password |

mysql service seems to be running fine on the required port as well.
C02WK0DQHV2J:Downloads svenkatesh$ sudo lsof -nPi -sTCP:LISTEN | grep sql
Password:
mysqld    15390         _mysql   19u  IPv6 0xf35d852c37f052c1      0t0  TCP *:3306 (LISTEN)
mysqld    15390         _mysql   22u  IPv6 0xf35d852c3792ce41      0t0  TCP *:33060 (LISTEN)

C02WK0DQHV2J:Downloads svenkatesh$ sudo netstat -an | grep -i sql
f35d852c413e4b61 stream      0      0 f35d852c463e3ca9                0                0                0 /tmp/mysqlx.sock
f35d852c413e65f1 stream      0      0 f35d852c44094009                0                0                0 /tmp/mysql.sock

"
"python has stopped working,while opening spyder through anaconda navigator","
Problem signature:
Problem Event Name: BEX
  Application Name: python.exe
  Application Version:  3.5.5150.1013
  Application Timestamp:    5ac89fa3
  Fault Module Name:    StackHash_0a9e
  Fault Module Version: 0.0.0.0
  Fault Module Timestamp:   00000000
  Exception Offset: 00000000
  Exception Code:   c0000005
  Exception Data:   00000008
  OS Version:   6.1.7601.2.1.0.256.1
  Locale ID:    1033
  Additional Information 1: 0a9e
  Additional Information 2: 0a9e372d3b4ad19135b953a78882e789
  Additional Information 3: 0a9e
  Additional Information 4: 0a9e372d3b4ad19135b953a78882e789

spyder launch may have produced error

[5656:5552:0622/121946.859:ERROR:gles2_cmd_decoder.cc(2439)]
  [GroupMarkerNotSet(crbug.com/242999)!:54C80215]GL ERROR
  :GL_INVALID_ENUM : BackFramebuffer::Create: <- error from previous GL
  command

"
LightGBM: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(),"
I was running lightgbm with categorical features:
X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.3)

train_data = lgb.Dataset(X_train, label=y_train, feature_name=X_train.columns, 
                                  categorical_feature=cat_features)

test_data = lgb.Dataset(X_test, label=y_train, reference=train_data)

param = {'num_trees': 4000, 'objective':'binary', 'metric': 'auc'}
bst = lgb.train(param, train_data, valid_sets=[test_data], early_stopping_rounds=100)

Turns out the Error: 

if self.handle is not None and feature_name is not None and
  feature_name != 'auto':
ValueError: The truth value of an array with more than one element is
  ambiguous. Use a.any() or a.all()

I checked the other similar errors on stackoverflow mostly related to numpy, and I then checked documentation and tried to replace my categorical_feature with index like [0, 2, 5, ...](my original was column names of categorical features), still the same error.
I also tried replacing label with the column index, still error.
Anyone could help? Thanks in advance.
"
python selenium scraping of flash,"
is it possible to scrape data from flash videos with python selenium? this is an example web page
i would like to get the 'bull ratio' data, which is included in the second chart (green and red bar), current value is around 36 %.
i enabled flash in chrome, so the page loads correctly, but can't get the desired number.
"
Python lambda doesn't remember argument in for loop [duplicate],"

Possible Duplicate:
Use value of variable in lambda expression 

I'm working with python and trying to isolate a problem I had with lambda functions.
From the following code I was expecting to create two lambda functions, each getting a different x, and the output should be
1
2
but the output is
2
2
Why? 
And how can I make two different functions? Using def?
def main():
    d = {}
    for x in [1,2]:
        d[x] = lambda : print(x)

    d[1]()
    d[2]()


if __name__ == '__main__':
    main()

"
Tornado websocket messages aren't receiving,"
I have a very simple setup inspired by this question: Tornado - Listen to multiple clients simultaneously over websockets
Essentially, I have one Websocket Handler that may connect to many websocket clients. Then I have another websocket handler 'DataHandler' that will broadcast a message everytime it receives a message. 
So I made a global list of TestHandler instances and use it to broadcast messages to all the instances
ws_clients = []

class TestHandler(tornado.websocket.WebSocketHandler):
    def open(self):
        print('open test!')
        ws_clients.append(self)
        self.random_number = random.randint(0, 101)

    def on_message(self, message):
        print(message)

        print('received', message, self, self.random_number)
        self.write_message('Message received')

    def on_close(self):
        print('closed')


class DataHandler(tornado.websocket.WebSocketHandler):
    def open(self):
        print('data open!')

    def on_message(self, message):
        for c in ws_clients:
            c.write_message('hello!')


class Application(tornado.web.Application):
    def __init__(self):
        handlers = [
            (r""/test_service/"", TestHandler),
            (r""/data/"", DataHandler),
            (r""/"", httpHandler)
        ]

        tornado.web.Application.__init__(self, handlers)


ws_app = Application()
ws_app.listen(8000)
tornado.ioloop.IOLoop.instance().start()

TestHandler can receive messages fine through the address ws://127.0.0.1/test_service/ and DataHandler can receive messages fine through the address ws://127.0.0.1/data/ but whenever I loop through ws_clients, I never receive any messages on TestHandler.
Am I doing something wrong?
"
Unable to create folder on apache server,"
I want to create a folder on apache
#!/usr/bin/python
import cgi, os
import cgitb; cgitb.enable()
import subprocess
import sys

if not os.path.exists(""/home/suni""):
        os.makedirs(""/home/suni"")

But i am unable to do that and facing this error
<type 'exceptions.OSError'>: [Errno 13] Permission denied: '/home/suni' 
      args = (13, 'Permission denied') 
      errno = 13 
      filename = '/home/suni' 
      message = '' 
      strerror = 'Permission denied'

i have tried chmod 777 /home
still facing the error.
"
regular expression,"
I need to match Regular expression for 
txt = ""orderType not in ('connect', 'Modify', 'random', 'more')""
correct data:
txt = ""orderType is in ('connect')""
txt = ""orderType not in ('connect', 'Modify')""
N number of items can be inside bracket, with quotes and comma seperated like above
Rest of all should not be matched, like below
txt = ""orderType not in ('connect', Modify, 'ran=dom', 'more')"" 
import re
pattern1 = '\w+\s+(?:is|not)\sin\s+\('
pattern2 = '\'\w+\''
pattern3 = '\s?,\s?'+pattern2+'+'
print(re.findall(pattern3, txt))
pattern6 = pattern1+pattern2
pattern5 = pattern1+pattern2+pattern3
pattern4 = (pattern2+ pattern3)  +'|'+ (pattern2 )
pattern = pattern5+ '|' + pattern6
print(re.findall(pattern,txt))

my output is [""orderType not in ('connect', 'Modify'""]
expected output should: orderType not in ('connect', 'Modify', 'random', 'more')
be entire line, I wont mind if it returns true for all matched and false for the rest
Thanks in advance
"
Error while generating screenshot in linux cannot connect to X server localhost:10.0,"
Problem:
Iam using PyQt4 library to generate screenshots of website.Code is presented below.When iam running for all the urls in a list the screenshots are getting generated for first 50-60 urls after that it is failing with the following error.
cannot connect to X server localhost:10.0
Code Snippet:
class Screenshot(QWebView):
    def __init__(self):
        self.app = QApplication(sys.argv)
        QWebView.__init__(self)
        self._loaded = False
        self.loadFinished.connect(self._loadFinished)

    def capture(self, url, output_file):
        _logger.info('Received url {}'.format(url))
        _start = time.time()
        try:
            #Check for http/https
            if url[0:3] == 'http' or url[0:4] == 'https':
                self.url = url
            else:
                url = 'http://' + url
            self.load(QUrl(url))
            self.wait_load(url)
            # set to webpage size
            frame = self.page().mainFrame()
            self.page().setViewportSize(frame.contentsSize())
            # render image
            image = QImage(self.page().viewportSize(), QImage.Format_ARGB32)
            painter = QPainter(image)
            frame.render(painter)
            painter.end()
            _logger.info('Saving screenshot {} for {}'.format(output_file,url))
            image.save(os.path.join(os.path.dirname(os.path.realpath(__file__)),'data',output_file))
        except Exception as e:
            _logger.error('Error in capturing screenshot {} - {}'.format(url,e))
        _end = time.time()
        _logger.info('Time took for processing url {} - {}'.format(url,_end - _start))

    def wait_load(self,url,delay=1,retry_count=60):
        # process app events until page loaded
        while not self._loaded and retry_count:
            _logger.info('wait_load for url {} retry_count {}'.format(url,retry_count))
            self.app.processEvents()
            time.sleep(delay)
            retry_count -=1
        _logger.info('wait_load for url {} expired'.format(url))
        self._loaded = False

    def _loadFinished(self, result):
        self._loaded = True

Any suggestions on how to solve this?Or is there any alternative approaches to do screenshot of urls.
System Config:
lscpu:
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                16
On-line CPU(s) list:   0-15
Thread(s) per core:    1
Core(s) per socket:    2
Socket(s):             8
NUMA node(s):          8
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 63
Model name:            Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz
Stepping:              2
CPU MHz:               2399.998
BogoMIPS:              4799.99
Hypervisor vendor:     VMware
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              15360K
NUMA node0 CPU(s):     0,1
NUMA node1 CPU(s):     2,3
NUMA node2 CPU(s):     4,5
NUMA node3 CPU(s):     6,7
NUMA node4 CPU(s):     8,9
NUMA node5 CPU(s):     10,11
NUMA node6 CPU(s):     12,13
NUMA node7 CPU(s):     14,15
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc aperfmperf pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm epb kaiser cqm_llc cqm_occup_llc dtherm ida arat pln pts

"
Selenium Webdriver get element (Python),"
I am making a bot that scrapes dictionary definitions. It first goes to this site: LINE Dictonary
It then looks up the word ""你好"", and there following results show:

My goal is to have the bot click on the first result. However, how would I do that using Selenium Webdriver?

Here is a snippet of what the HTML looks like for the result. However, if you would like to see all of it, please go to LINE Dictonary's website and type in ""你好"". Thank you!
"
Change “activate_url” in email confirmation in django allauth,"
The email is being sent from the template email_confirmation_message.txt
{% load account %}{% user_display user as user_display %}{% load i18n %}{% autoescape off %}{% blocktrans with site_name=current_site.name site_domain=current_site.domain %}Hello from {{ site_name }}!

You're receiving this e-mail because user {{ user_display }} has given yours as an e-mail address to connect their account.

To confirm this is correct, go to {{ activate_url }}
{% endblocktrans %}{% endautoescape %}
{% blocktrans with site_name=current_site.name site_domain=current_site.domain %}Thank you from {{ site_name }}!
{{ site_domain }}{% endblocktrans %}

The activate_url is sending link as http://web/en/accounts/confirm-email/Mjc:1fWFRw:q-ScRb0Wnsrrem9N7pB9iLOuPOM/  where web is my conatiner hostname. How can I change this to custom domain name without having to change my conatiner hostname. I am using django-allauth==0.33.0
"
"Converting docx to pdf with pure python (on linux, without libreoffice)","
I'm dealing with a problem trying to develop a web-app, part of which converts uploaded docx files to pdf files (after some processing). With python-docx and other methods, I do not require a windows machine with word installed, or even libreoffice on linux, for most of the processing (my web server is pythonanywhere - linux but without libreoffice and without sudo or apt install permissions). But converting to pdf seems to require one of those. From exploring questions here and elsewhere, this is what I have so far:
import subprocess

try:
    from comtypes import client
except ImportError:
    client = None

def doc2pdf(doc):
    """"""
    convert a doc/docx document to pdf format
    :param doc: path to document
    """"""
    doc = os.path.abspath(doc) # bugfix - searching files in windows/system32
    if client is None:
        return doc2pdf_linux(doc)
    name, ext = os.path.splitext(doc)
    try:
        word = client.CreateObject('Word.Application')
        worddoc = word.Documents.Open(doc)
        worddoc.SaveAs(name + '.pdf', FileFormat=17)
    except Exception:
        raise
    finally:
        worddoc.Close()
        word.Quit()


def doc2pdf_linux(doc):
    """"""
    convert a doc/docx document to pdf format (linux only, requires libreoffice)
    :param doc: path to document
    """"""
    cmd = 'libreoffice --convert-to pdf'.split() + [doc]
    p = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    p.wait(timeout=10)
    stdout, stderr = p.communicate()
    if stderr:
        raise subprocess.SubprocessError(stderr)

As you can see, one method requires comtypes, another requires libreoffice as a subprocess. Other than switching to a more sophisticated hosting server, is there any solution?
"
Make a multi page gui using pyqt5,"
I have made several .ui files in qtDesigner.

I would like to make a fullscreen app for a raspberry pi written in python.
I would like the entire screen to be occupied by the content of one of my .ui files at a time.

My problem is that I can't find a way to bring up the homescreen.ui (or homescreen.py) file and then use one of the buttons on the homescreen.ui file to switch the current screen to settings.ui
Example:
This is going to be a fairly large ui with many screens that i would like to switch between and add many functions per screen.
If there is a better gui library to use for this project I am open to trying something else.
"
Mocking functions retrieved from __getattr__ method,"
I'm automating some repository operations and I'm using GitPython for this job. Let's simplify things and assume I'd like to assert whether my function called pull method on the repository. Code below:
from pytest_mock import MockFixture
from git import Git, Repo

repo = Repo('/Users/Jatimir/path/to/repo')

def pull() -> None:
    repo.git.pull()

However, I noticed that Git class is somewhat special and doesn't implement pull. Instead, it ""delegates"" all the traffic to __getattr__ which uses another method that does the job.
def __getattr__(self, name):
    ...
    return lambda *args, **kwargs: self._call_process(name, *args, **kwargs)

My question is how to approach testing this? I'm using pytest with pytest-mock that provides a mocker fixture and here are my attempts:
def test_pull1(mocker: MockFixture) -> None:
    pull_mock = mocker.MagicMock(name='pull')
    getattr_mock = mocker.MagicMock(name='__getattr__', return_value=pull_mock)

    mocker.patch.object(Git, '__getattr__', getattr_mock)
    pull()
    pull_mock.assert_called_once_with()


def test_pull2(mocker: MockFixture) -> None:
    pull_mock = mocker.Mock(name='pull')

    def __getattr__(self, name):
        if name == 'pull':
            return pull_mock

    mocker.patch.object(Git, '__getattr__', __getattr__)
    pull()
    pull_mock.assert_called_once_with()

They both work, but I feel like there is a better way and maybe my approach to testing this is wrong.
"
Empty data frame when it is not supposed to be empty,"
the code:
s = pd.DataFrame(df)
print(s.head(3))
print(s.loc[s.Product=='A'])

output:
  Product  Sales  Date 
---------------------------------------------------------------------
  0    A      20   2017-5-16 
  1    A      60   2016-6-16 
  2    A      30   2015-6-16

Empty DataFrame
Columns: [Product, Sales, Date]
Index: [ ]

Why is the data frame empty in this case? The iloc function works perfectly, but is a hassle and I don't want to use it for 185 products with 31,000 data points. 
Yes, the name of the product is typed in right. 
I tried the isin function as well which threw the same error.

Note: The type is Series, but I converted it to a DataFrame. However, even after the conversion, it shows the type to be Series. This might be a key insight as to what is wrong. 
"
Running Python code on RStudio (via Flexdashboard),"
I have been using Flexdashboard recently through R Studio and I can run it using pure R code (to build dashboards). However, I am also trying to run Python code on Flexdashboard. Is there a way to get it working ?
Essentially, I'm trying to build a dashboard which outputs visuals through a mix of R and Python codes.
"
Python - Parents and Multithreading issues,"
I am learning PyQT and Multithreading, and I think I need help here. In my code, I have a a class Tool, and a thread TaskThread. From the thread, I call the function dummy_function which perform a proccess being displayed in the GUI by a progress-bar.
When dummy_function finishes, I want to print a message in the GUI, by calling self.outputText.setText(""Hello""). However, I get the following error:

AttributeError: 'TaskThread' object has no attribute 'outputText'

Code:
import sys
import datetime
import time

from PyQt4 import QtCore, QtGui, uic
from PyQt4.QtCore import *
from PyQt4.QtGui import *

# Link to GUI
qtCreatorFile = ""GUInterface.ui""

Ui_MainWindow, QtBaseClass = uic.loadUiType(qtCreatorFile)

def time_converter_to_unix(start_datetime, end_datetime):
    # Convert QTimeEdit to UNIX Timestamp (int, msec included), and then to float
    start_datetime_unix_int = start_datetime.toMSecsSinceEpoch ()
    start_datetime_unix = (float(start_datetime_unix_int) / 1000)

    end_datetime_unix_int = end_datetime.toMSecsSinceEpoch ()
    end_datetime_unix = (float(end_datetime_unix_int) / 1000)

    return start_datetime_unix, end_datetime_unix

def dummy_function(self, start_datetime_unix, end_datetime_unix): 
    # Dummy function, just to simulate a task. It takes parameters just for testing.
    result = start_datetime_unix * end_datetime_unix 

    # Pre-steps for mapping from one range to other (progress-bar)
    OldMax = 99999
    OldMin = 1
    NewMax = 100
    NewMin = 1

    OldRange = (OldMax - OldMin)
    NewRange = (NewMax - NewMin)    

    u = 1
    for i in range (OldMax):
        u = i*2

        OldValue = i
        print OldValue
        NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin
        print ""NEW VALUE: "", NewValue

        self.emit(QtCore.SIGNAL('CPU_VALUE'), NewValue)

    self.outputText.setText(""Hello"")

class Tool(QtGui.QMainWindow, Ui_MainWindow):
    def __init__(self, parent = None):

        # Setting-ip UI
        QtGui.QMainWindow.__init__(self)
        Ui_MainWindow.__init__(self)
        self.setupUi(self)

        # Button Action
        self.runButton.clicked.connect(self.onStart)

        # Progress Bar and Label. At the begining, the bar is at 0
        self.progressBar.setValue(0)
        self.progressBar.setRange(0,100)
        self.resultLabel.setText(""Waiting..."")  

        ####################################
        #TEST: Thread for progress bar
        self.myLongTask = TaskThread()
        self.connect(self.myLongTask, QtCore.SIGNAL('CPU_VALUE'), self.onProgress)
        self.myLongTask.taskFinished.connect(self.onFinished)
        ####################################        

    def onStart(self):    

        self.progressBar.reset()
        self.resultLabel.setText(""In progress..."")        
        print ""(onStart)In progress mode executed""

        print ""(onStart)INITIALIZE THREAD""
        self.myLongTask.start()     
        print ""(onStart)THREAD EXECUTED""

        self.myLongTask.start_dt = self.startTime.dateTime() # <----
        self.myLongTask.end_dt = self.endTime.dateTime()     # <----

    def onProgress(self, i):
        self.progressBar.setValue(i)        

    def onFinished(self):
        # Stop the pulsation when the thread has finished
        print ""(onFinished) executed""
        self.progressBar.setRange(0,1)
        self.progressBar.setValue(1)
        self.resultLabel.setText(""Done"")        

class TaskThread(QtCore.QThread):  
      taskFinished = QtCore.pyqtSignal()

    def __init__(self):
        QtCore.QThread.__init__(self)

    def __del__(self):
        self.wait()

    def run(self):  
        # First, we read the times from the QDateTime elements in the interface      
        print ""Getting times...""
        start_datetime_unix, end_datetime_unix = time_converter_to_unix(self.start_dt, self.end_dt)

        # Then, we put these values in my_function
        print ""Executing function...""
        dummy_function(self, start_datetime_unix, end_datetime_unix) 

        # To finish, we execute onFinished.
        print ""Finishing thread...""
        self.taskFinished.emit()

def main():
    app = QtGui.QApplication(sys.argv)
    window = Tool()
    window.show()
    sys.exit(app.exec_())

if __name__ == '__main__':
    main()

The first thing I tried, it was to add the following line to my onStart function:
...
self.myLongTask.output_dt = self.outputText
...

Then, in the TaskThread, I change the call of dummy_function by: 
...
dummy_function(self, start_datetime_unix, end_datetime_unix, self.output_dt) 
...

To finish, I change my dummy_function by adding this new parameter:
def dummy_function(self, start_datetime_unix, end_datetime_unix, output_text): 
    # Dummy function, just to simulate a task. It takes parameters just for testing.
    result = start_datetime_unix * end_datetime_unix 

    # Pre-steps for mapping from one range to other (progress-bar)
    OldMax = 99999
    OldMin = 1
    NewMax = 100
    NewMin = 1

    OldRange = (OldMax - OldMin)
    NewRange = (NewMax - NewMin)    

    u = 1
    for i in range (OldMax):
        u = i*2

        OldValue = i
        print OldValue
        NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin
        print ""NEW VALUE: "", NewValue

        self.emit(QtCore.SIGNAL('CPU_VALUE'), NewValue)

    output_text.setText(""Hello"")

Now, when it reaches the end, the program just closes showing the following error:

QObject: Cannot create children for a parent that is in a different thread.
  (Parent is QTextDocument(0xf13a60), parent's thread is QThread(0xc6f180), current thread is TaskThread(0xee0740)

In order to try the program, I uplodaed the GUInterface, so you can try it if necessary.
"
Swig: using c++ STL complex in Python,"
I want to use c++ complex type in python. I tried the following Swig interface file:
%module example
%include <std_complex.i>
%template(complexf) std::complex<float>;

It returned this error:
example.i:3: Error: Template 'complex' undefined.

If I include c++ header file  manually,
%module example
%include <std_complex.i>

%include ""/usr/include/c++/7/complex""
%template(complexf) std::complex<float>;

I would get another error:
/usr/include/c++/7/complex:50: Error: Syntax error in input(1).

So I am wondering how to use c++ complex with SWIG correctly?
"
How can I pop up the widget into QMdiArea,"
I made MainWindow and Widget with QT Designer. 
I wanna use Widget as subwindows of mdi. 
I tested the below code. 
When I click the menu to stop and finish.
How can I do?
import sys

from PyQt5.QtWidgets import *
from PyQt5 import uic

WCCMainWindow_Form_Class = uic.loadUiType(""WCCMainWindows.ui"")[0]
Form_DS_Form_Class = uic.loadUiType(""Form_DS.ui"")[0]

class WCCMainWindow(QMainWindow, WCCMainWindow_Form_Class):
    def __init__(self):
        super().__init__()
        self.setupUi(self)
        self.mdi = QMdiArea()
        self.setCentralWidget(self.mdi)

        self.Action_IGroup.triggered.connect(self.subWinIGroup)

    def subWinIGroup(self):
        sub = DSSubWindow(self)
        self.mdi.addSubWindow(sub)
        sub.show()

if __name__ == ""__main__"":
    app = QApplication(sys.argv)
    wccMainWindow = WCCMainWindow()
    wccMainWindow.show()
    app.exec_()

class DSSubWindow(QWidget, Form_DemandSupply_Form_Class):
    def __init__(self):
        super().__init__()
        self.setupUi(self)

"
How to execute Python Script file from c#.net,"
I'm completely new for both python and ironpython,help me to execute python scripts from c# without using ProcessStartInfo 
My python Code.
import sys 

from PIL import Image

img = Image.open(""c:\python27\Image.png"")

img.show()

My C# Code is,
  var engine = Python.CreateEngine();
            var searchPaths = engine.GetSearchPaths();
            searchPaths.Add(@""C:\Python27\Lib"");
            engine.SetSearchPaths(searchPaths);
            var scope = engine.CreateScope();
            ScriptSource source = engine.CreateScriptSourceFromFile(@""C:\Python27\Sample.py"");

            source.Execute(scope);

when i run above code i'm getting Below Error.
An unhandled exception of type 'IronPython.Runtime.Exceptions.ImportException' occurred in Microsoft.Dynamic.dll
Additional information: No module named PIL.
"
How to create a scatter plot with timestamps,"
How to produce a scatter plot using timestamps?
Below is an example but I get an error ValueError: First argument must be a sequence
import pandas as pd
import matplotlib.pyplot as plt

d = ({
    'A' : ['08:00:00','08:10:00'],
    'B' : ['1','2'],           
    })

df = pd.DataFrame(data=d)

fig = plt.figure()

x = df['A']
y = df['B']

plt.scatter(x,y)

If I convert the timestamps to total seconds it works:
df['A'] = pd.to_timedelta(df['A'], errors=""coerce"").dt.total_seconds()

But I'd like 24hr time to be on the x-axis, rather than total seconds.
"
Using T-test for two-sided one sample,"
I have 60 observations of one sample with several variables. In order to see whether or not the individuals in a sample are normally distributed, is that correct to use T-test for one sample? I mean using it for each of the variables in a sample? 
I really appreciate your help.
"
Change default location log file generated by logger in python,"
I am using logger in my python source code, and want  to create logs on specific location but python logging module creates the log files at the default place i.e. from where it is executed.
Is there is any way to change this default location?
below is my configuration 
  import logging
  logger = logging.getLogger(__name__)
  logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename='testGene.log, filemode='w')

"
Django: How to get a distinct Parent List from a Child Queryset related to the User?,"
These are my models where Apps have menu_items:
class App(models.Model):
    name = models.CharField(max_length=50)

class Menu(models.Model):
    name = models.CharField(max_length=120)
    app = models.ForeignKey(App, on_delete=models.PROTECT)
    url = models.CharField(max_length=120, null=True)

    class Meta:
        ordering = ['name', 'app']

This is my View, is there any way this can be optimized?
class AboutView(TemplateView):
    template_name = 'about.html'

    def get_context_data(self, *args, **kwargs):
        context = super(AboutView, self).get_context_data(*args, **kwargs)
        menu_items = Access.objects.filter(user__id=self.request.user.id)
        applist = []
        for m in menu_items:
            applist.append(m.menu.app.name)
        apps = App.objects.filter(name__in=applist)
        context = {
            ""menu_items"": menu_items,
            ""apps"": apps
        }
        return context

This is my template, I rearrange the menu_items under each app.            
<ul class=""navbar-nav ml-auto"">
                {% for app in apps %}
                    <li class=""nav-item dropdown"">
                        <a class=""nav-link"" href=""#"" id=""appMenu"" data-toggle=""dropdown"" aria-haspopup=""true"" aria-expanded=""false"">
                            {{ app.name }}
                        </a>
                        <div class=""dropdown-menu dropdown-menu-left"" aria-labelledby=""appMenu"">
                            {% for item in menu_items %}
                                {% if item.menu.app.name == app.name %}
                                    <a class=""dropdown-item"" href=""{{ item.menu.url }}"">{{ item.menu.name }}</a>
                                    {# href=""{{ item.menu.url }} #}
                                {% endif %}
                            {% endfor %}
                        </div>
                    </li>
                {% endfor %}
            </ul>

I was hoping there was a better way in doing this. Thank you in advance.
"
Error: “ValueError: No JSON object could be decoded”,"
Following is my Simple JSON code which is showing the 

error: ""ValueError: No JSON object could be decoded""

import csv, json

infile = open(""01_ZD_20180501_00407_001.json"",""r"")
outfile = open(""01_ZD_20180501_00407_001.csv"",""w"")

writer = csv.writer(outfile)

for row in json.loads(infile.read()):
    writer.writerow(row)

errors show like below

"
partial replacement of content of one file with the content of another file,"
file 1 has a format like: 
root < p1:v1 >< p2:v2 >< p3:v3 >< p4:v4 >< p5:v5 >

and file 2 has 4 columns separated by a tab. the valus of columns are the values for root, p3, p2, p4 respectively of file 1. I need to replace each line of file1 with the respective values of parameters in file 2. and save it in a third file.
total lines in file1 and file2 are 9 and 2000 respecively.
i need to write a python program for it.
"
